{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1eqG4eT6uM+mpzp3relSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanda-Bhavesh/NLP-Assignment/blob/main/customer_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_L3KehUBEUi"
      },
      "outputs": [],
      "source": [
        "# Import necessary Libraries\n",
        "import pandas as pd import numpy as np import re import nitk\n",
        "from nltk.corpus import stopwords from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn. linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision score, recall_score, f1_\n",
        "import matplotlib.pyplot as plt import seaborn as sns\n",
        "# NLTK Downloads\n",
        "nltk.download (\"stopwords\")\n",
        "nltk.download (“wordnet”)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Collection - Load the dataset\n",
        "data = pd.read_csv(\"synthetic_sentiment_dataset.csv\")"
      ],
      "metadata": {
        "id": "ZcS2gNZHEdjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. data preprocessing techniques\n",
        "def clean_text(text):\n",
        "  \"\"\"remove spl characters,convert to lower case.\"\"\"\n",
        "  text = text.lower()\n",
        "  return re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "def tokenize_text(text):\n",
        "  \"\"\"split text into tokens.\"\"\"\n",
        "  return text.split()\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "  \"\"\"remove stopwords from tokens.\"\"\"\n",
        "  return [word for word in tokens if word not in stopwords.words(\"english\")]\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "  \"\"\"lemmatize tokens using WordNetLemmatizer.\"\"\"\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  #appply preprocessing functions\n",
        "  data[\"cleaned_text\"] = data[\"text\"].apply(clean_text)\n",
        "  data[\"tokens\"] = data[\"cleaned_text\"].apply(tokenize_text)\n",
        "  data[\"tokens\"] = data[\"tokens\"].apply(remove_stopwords)\n",
        "  data[\"tokens\"] = data[\"tokens\"].apply(lemmatize_tokens)\n",
        "  data[\"processed_text\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))"
      ],
      "metadata": {
        "id": "oj3y-gVeEdfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Visualizing Dataset and Key Stats\n",
        "print(\"Full Dataset with Preprocessing Steps:\")\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "HszmCaWzGkeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution visualization\n",
        "sentiment_counts = data[\"Sentiment\"].value_counts()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"viris\")\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment™)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uKr2gyoPGq2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text length distribution visualization\n",
        "data[\"Text_Length\"] = data[\"Processed_Text\"].apply(len)\n",
        "plt.\n",
        "sns.\n",
        "figure(figsize=(10, 6))\n",
        "histplot(data[\"Text_Length\"], kde=True, bins=30, color=\"blue\")\n",
        "plt.title(\"Distribution of Processed Text Lengths\")\n",
        "plt.xlabel(\"Text Length™)\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0DqGzMNaGzz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. X =\n",
        "y = frequency = ] g\n",
        "50\n",
        "o\n",
        "12 14 16 18 20\n",
        "Text Length\n",
        "Split Data into Training and Testing Sets\n",
        "data[\"Processed_Text\"] # Features (processed reviews)\n",
        "data[\"Sentiment\"] # Labels (sentiment)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Convert text into numerical features using TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "q3zTARMdG-41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Model Development & Evaluation\n",
        "models = {\n",
        "\"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\"Logistic Regression”: LogisticRegression(random_state=42),\n",
        "\"SVM\": SVC(kernel=\"linear\", random_state=42)\n",
        "}\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "\n",
        "print(“\"Model Performance Evaluation:\")\n",
        "\n",
        "# Evaluate models and select the best one\n",
        "for model_name, model in models.items();\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test, y pred, average=\"weighted\")\n",
        "f1 = fl1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"\\n{model_name} performance:\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\n",
        "print(f\"Fl-Score: {f1:.2f}\")\n",
        "\n",
        "# Track the best model\n",
        "if accuracy > best_accuracy:\n",
        "best_accuracy = accuracy\n",
        "best_model = model"
      ],
      "metadata": {
        "id": "bZkrSve0HIkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Confusion Matrix for Best Model\n",
        "print(\"\\nBest Model: \", best_model)\n",
        "best_model_predictions = best_model.predict(X_test_tfidf)\n",
        "conf_matrix = confusion_matrix(y_test, best_model_predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.title(\"Confusion Matrix for Best Model\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "za8x2dZ1IiGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Display Dataset with Predicted Sentiment\n",
        "data[\"Predicted_Sentiment\"] = best_model.predict(vectorizer.transform(data[\"Processed_Text\"]))\n",
        "print(\"\\nDataset with Predicted Sentiment:\")\n",
        "print(data)\n",
        "\n",
        "# Save the final dataset with predictions\n",
        "data.to_csv(\"final_processed_dataset_with_predictions.csv\", index=False)\n",
        "print(\"Final processed dataset with predictions has been saved successfully!\")"
      ],
      "metadata": {
        "id": "mkR_FQwGJKR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}