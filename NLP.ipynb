{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyL1z25QOfftuK4V09pHa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chanda-Bhavesh/NLP-Assignment/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSN1B5YU0L4I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CS8FQqIL1Vj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Correct the Search Query\n",
        "import re\n",
        "import json\n",
        "import zlib\n",
        "from difflib import get_close_matches\n",
        "\n",
        "def load_corpus():\n",
        "    \"\"\"Loads a prebuilt word corpus and country names.\"\"\"\n",
        "\n",
        "    word_list = [\n",
        "        \"going\", \"to\", \"china\", \"who\", \"was\", \"the\", \"first\", \"president\", \"of\", \"india\", \"winner\", \"match\", \"food\", \"in\", \"america\"\n",
        "    ]\n",
        "    country_names = [\"india\", \"china\", \"usa\", \"america\"]\n",
        "\n",
        "\n",
        "    corpus = set(word_list + country_names)\n",
        "\n",
        "\n",
        "    compressed_corpus = zlib.compress(json.dumps(list(corpus)).encode())\n",
        "    return compressed_corpus\n",
        "\n",
        "def decompress_corpus(compressed_corpus):\n",
        "    \"\"\"Decompresses and loads the word corpus.\"\"\"\n",
        "    return set(json.loads(zlib.decompress(compressed_corpus).decode()))\n",
        "\n",
        "def correct_word(word, corpus):\n",
        "    \"\"\"Corrects a single word using fuzzy matching.\"\"\"\n",
        "    matches = get_close_matches(word, corpus, n=1, cutoff=0.8)\n",
        "    return matches[0] if matches else word\n",
        "\n",
        "def segment_text(text, corpus):\n",
        "    \"\"\"Handles cases where spaces are missing between words.\"\"\"\n",
        "    for i in range(1, len(text)):\n",
        "        left, right = text[:i], text[i:]\n",
        "        if left in corpus and right in corpus:\n",
        "            return f\"{left} {right}\"\n",
        "    return text\n",
        "\n",
        "def correct_query(query, corpus):\n",
        "    \"\"\"Corrects the entire query.\"\"\"\n",
        "    words = query.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    for word in words:\n",
        "\n",
        "        corrected_word = correct_word(word, corpus)\n",
        "\n",
        "\n",
        "        if corrected_word == word:\n",
        "            corrected_word = segment_text(word, corpus)\n",
        "        corrected_words.append(corrected_word)\n",
        "\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def main():\n",
        "\n",
        "    compressed_corpus = load_corpus()\n",
        "    corpus = decompress_corpus(compressed_corpus)\n",
        "\n",
        "\n",
        "    n = int(input())\n",
        "    queries = [input().strip() for _ in range(n)]\n",
        "\n",
        "\n",
        "    results = [correct_query(query, corpus) for query in queries]\n",
        "\n",
        "\n",
        "    print(\"\\n\".join(results))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rZXG9Ur31Bwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Deterministic Url and HashTag Segmentation\n",
        "import re\n",
        "\n",
        "def load_words(file_path):\n",
        "    \"\"\"Load the list of words from the words.txt file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            return set(word.strip().lower() for word in file.readlines())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{file_path}' was not found.\")\n",
        "        return set()\n",
        "\n",
        "def segment_string(input_string, words):\n",
        "    \"\"\"Segment the input string into valid words based on the dictionary.\"\"\"\n",
        "    def helper(s):\n",
        "        if not s:\n",
        "            return []\n",
        "        for i in range(len(s), 0, -1):\n",
        "            prefix = s[:i]\n",
        "            if prefix in words or re.match(r\"^\\d+(\\.\\d+)?$\", prefix):\n",
        "                rest = helper(s[i:])\n",
        "                if rest is not None:\n",
        "                    return [prefix] + rest\n",
        "        return None\n",
        "\n",
        "    result = helper(input_string)\n",
        "    return result if result else [input_string]\n",
        "\n",
        "def clean_input(input_string):\n",
        "    \"\"\"Clean the input string by removing www, extensions, or hashtags.\"\"\"\n",
        "    if input_string.startswith(\"www.\"):\n",
        "        input_string = input_string[4:]\n",
        "\n",
        "    input_string = re.sub(r\"\\.(com|net|org|edu|gov|us|ru|cn|tv|co.uk|co|io|in)$\", \"\", input_string)\n",
        "\n",
        "    if input_string.startswith(\"#\"):\n",
        "        input_string = input_string[1:]\n",
        "    return input_string\n",
        "\n",
        "def main():\n",
        "\n",
        "    words = load_words(\"words.txt\")\n",
        "\n",
        "    if not words:\n",
        "        return\n",
        "\n",
        "\n",
        "    n = int(input())\n",
        "    inputs = [input().strip() for _ in range(n)]\n",
        "\n",
        "\n",
        "    results = []\n",
        "    for input_string in inputs:\n",
        "        cleaned_input = clean_input(input_string)\n",
        "        segmented = segment_string(cleaned_input.lower(), words)\n",
        "        results.append(\" \".join(segmented))\n",
        "\n",
        "\n",
        "    print(\"\\n\".join(results))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "cQVoDIW61nT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Disambiguation: Mouse vs Mouse\n",
        "import re\n",
        "\n",
        "def disambiguate_mouse(sentence):\n",
        "\n",
        "    animal_keywords = ['tail', 'squeak', 'genome', 'postnatal', 'environmental', 'temperature', 'scurried', 'rodent']\n",
        "    computer_mouse_keywords = ['input device', 'click', 'cursor', 'select', 'pointer', 'scroll', 'drag']\n",
        "\n",
        "\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "\n",
        "    for keyword in animal_keywords:\n",
        "        if keyword in sentence:\n",
        "            return \"animal\"\n",
        "\n",
        "    for keyword in computer_mouse_keywords:\n",
        "        if keyword in sentence:\n",
        "            return \"computer-mouse\"\n",
        "\n",
        "\n",
        "    return \"animal\"\n",
        "\n",
        "def main():\n",
        "\n",
        "    n = int(input())\n",
        "\n",
        "\n",
        "    for _ in range(n):\n",
        "        sentence = input().strip()\n",
        "        result = disambiguate_mouse(sentence)\n",
        "        print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7_wDHiko1peL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Language Detection\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "language_stopwords = {\n",
        "    \"English\": [\"the\", \"and\", \"is\", \"in\", \"it\", \"of\", \"to\", \"a\", \"that\", \"with\"],\n",
        "    \"French\": [\"le\", \"la\", \"les\", \"et\", \"de\", \"un\", \"une\", \"Ã \", \"que\", \"pour\"],\n",
        "    \"German\": [\"der\", \"die\", \"das\", \"und\", \"in\", \"zu\", \"mit\", \"von\", \"ist\", \"das\"],\n",
        "    \"Spanish\": [\"el\", \"la\", \"los\", \"las\", \"y\", \"en\", \"de\", \"que\", \"con\", \"para\"]\n",
        "}\n",
        "\n",
        "language_ngrams = {\n",
        "    \"English\": [\"th\", \"he\", \"in\", \"an\", \"nd\", \"on\", \"es\", \"is\", \"at\", \"or\"],\n",
        "    \"French\": [\"le\", \"de\", \"on\", \"ent\", \"es\", \"que\", \"un\", \"re\", \"nt\", \"en\"],\n",
        "    \"German\": [\"er\", \"en\", \"ch\", \"die\", \"der\", \"zu\", \"sch\", \"ist\", \"te\", \"und\"],\n",
        "    \"Spanish\": [\"la\", \"el\", \"de\", \"en\", \"es\", \"que\", \"se\", \"un\", \"con\", \"los\"]\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    return re.sub(r'[^\\x00-\\x7F]+', '', text).lower()\n",
        "\n",
        "def get_ngrams(text, n=2):\n",
        "\n",
        "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
        "\n",
        "def compare_ngrams(text_ngrams, language_ngrams):\n",
        "\n",
        "    text_ngram_count = Counter(text_ngrams)\n",
        "    language_ngram_count = Counter(language_ngrams)\n",
        "\n",
        "    score = 0\n",
        "    for ngram, count in text_ngram_count.items():\n",
        "        if ngram in language_ngram_count:\n",
        "            score += count * language_ngram_count[ngram]\n",
        "\n",
        "    return score\n",
        "\n",
        "def detect_language(text):\n",
        "\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    text_ngrams = get_ngrams(cleaned_text)\n",
        "\n",
        "    scores = {}\n",
        "    for language, ngrams in language_ngrams.items():\n",
        "        score = compare_ngrams(text_ngrams, ngrams)\n",
        "\n",
        "        stopwords = language_stopwords.get(language, [])\n",
        "        score += sum(1 for word in cleaned_text.split() if word in stopwords)\n",
        "        scores[language] = score\n",
        "\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "def main():\n",
        "\n",
        "    text = \"\"\n",
        "    try:\n",
        "        while True:\n",
        "            line = input().strip()\n",
        "            if line:\n",
        "                text += line + \" \"\n",
        "            else:\n",
        "                break\n",
        "    except EOFError:\n",
        "        pass\n",
        "\n",
        "\n",
        "    language = detect_language(text)\n",
        "\n",
        "\n",
        "    print(language)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pfNxje0X1uO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) The Missing Apostrophes\n",
        "import re\n",
        "\n",
        "def fix_apostrophes(text):\n",
        "    # Handle contractions\n",
        "    contractions = {\n",
        "        \"dont\": \"don't\", \"cant\": \"can't\", \"wont\": \"won't\", \"isnt\": \"isn't\", \"arent\": \"aren't\",\n",
        "        \"wasnt\": \"wasn't\", \"werent\": \"weren't\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\",\n",
        "        \"hadnt\": \"hadn't\", \"couldnt\": \"couldn't\", \"shouldnt\": \"shouldn't\", \"wouldnt\": \"wouldn't\",\n",
        "        \"didnt\": \"didn't\", \"im\": \"I'm\", \"ive\": \"I've\", \"youve\": \"you've\", \"theyve\": \"they've\",\n",
        "        \"shes\": \"she's\", \"hes\": \"he's\", \"its\": \"it's\", \"id\": \"I'd\", \"ill\": \"I'll\", \"ill\": \"I'll\",\n",
        "        \"ive\": \"I've\", \"youd\": \"you'd\", \"wed\": \"we'd\", \"they'd\": \"they'd\", \"theres\": \"there's\",\n",
        "        \"whats\": \"what's\", \"heres\": \"here's\", \"lets\": \"let's\", \"thats\": \"that's\"\n",
        "    }\n",
        "\n",
        "    # Replace contractions with apostrophes\n",
        "    for word, replacement in contractions.items():\n",
        "        text = re.sub(r'\\b' + word + r'\\b', replacement, text)\n",
        "\n",
        "    # Fix possessive cases, i.e. partys -> party's\n",
        "    text = re.sub(r'(\\b\\w+)\\s+(?:partys|candidates|voters)\\b', r'\\1\\'s', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"At a news conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on his previous trip into space in 2010 \"I even asked our psychological support folks to send me a calendar with photographs of nature, of rivers, of woods, of lakes.\"\n",
        "Kelly was asked if hed miss his twin brother Mark, who also was an astronaut.\n",
        "\"Were used to this kind of thing,\" he said. \"Ive gone longer without seeing him and it was great.\"\n",
        "The mission wont be the longest time that a human has spent in space - four Russians spent a year or more aboard the Soviet-built Mir space station in the 1990s.\"\"\"\n",
        "\n",
        "# Apply function\n",
        "fixed_text = fix_apostrophes(text)\n",
        "\n",
        "# Output the fixed text\n",
        "print(fixed_text)"
      ],
      "metadata": {
        "id": "lOB_XqO11zaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Segment the Twitter Hashtags\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "COMMON_WORDS = set([\n",
        "    \"we\", \"are\", \"the\", \"people\", \"mention\", \"your\", \"faves\", \"now\", \"playing\",\n",
        "    \"the\", \"walking\", \"dead\", \"follow\", \"me\"\n",
        "])\n",
        "\n",
        "\n",
        "def is_word(word):\n",
        "    return word.lower() in COMMON_WORDS\n",
        "\n",
        "\n",
        "def split_hashtag(hashtag):\n",
        "    n = len(hashtag)\n",
        "    dp = [False] * (n + 1)\n",
        "    dp[0] = True\n",
        "    split_points = [-1] * (n + 1)\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(i):\n",
        "            if dp[j] and is_word(hashtag[j:i]):\n",
        "                dp[i] = True\n",
        "                split_points[i] = j\n",
        "                break\n",
        "\n",
        "    if not dp[-1]:\n",
        "        return hashtag\n",
        "    words = []\n",
        "    idx = n\n",
        "    while idx > 0:\n",
        "        prev_idx = split_points[idx]\n",
        "        words.append(hashtag[prev_idx:idx])\n",
        "        idx = prev_idx\n",
        "\n",
        "    return \" \".join(reversed(words))\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    n = int(input())\n",
        "    hashtags = [input().strip() for _ in range(n)]\n",
        "\n",
        "\n",
        "    for hashtag in hashtags:\n",
        "        print(split_hashtag(hashtag))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Y-UfSeHH123E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Expand the Acronyms\n",
        "import re\n",
        "\n",
        "def extract_acronyms_and_expansions(snippets):\n",
        "    \"\"\"Extract acronyms and their expansions from the provided snippets.\"\"\"\n",
        "    acronym_dict = {}\n",
        "    for snippet in snippets:\n",
        "        matches = re.findall(r'\\b([A-Z][A-Z0-9]+)\\b|\\(([A-Z][A-Z0-9]+)\\)', snippet)\n",
        "        for match in matches:\n",
        "            acronym = match[0] if match[0] else match[1]\n",
        "            pattern = rf\"([\\w\\s]+)\\s+\\(?{acronym}\\)?\"\n",
        "            expansion_match = re.search(pattern, snippet)\n",
        "            if expansion_match:\n",
        "                expansion = expansion_match.group(1).strip()\n",
        "                acronym_dict[acronym] = expansion\n",
        "    return acronym_dict\n",
        "\n",
        "def main():\n",
        "    n = int(input().strip())\n",
        "    snippets = [input().strip() for _ in range(n)]\n",
        "    tests = [input().strip() for _ in range(n)]\n",
        "\n",
        "    acronym_dict = extract_acronyms_and_expansions(snippets)\n",
        "\n",
        "\n",
        "    for test in tests:\n",
        "        print(acronym_dict.get(test, \"Not Found\"))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "I9-VRmVf2LwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) A Text-Processing Warmup\n",
        "import re\n",
        "\n",
        "def count_articles_and_dates(text):\n",
        "    \"\"\"Count the occurrences of articles and dates in the given text.\"\"\"\n",
        "    a_pattern = r'\\ba\\b'\n",
        "    an_pattern = r'\\ban\\b'\n",
        "    the_pattern = r'\\bthe\\b'\n",
        "\n",
        "    date_patterns = [\n",
        "        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n",
        "        r'\\b\\d{1,2}(st|nd|rd|th)?\\s+[A-Za-z]+\\s+\\d{2,4}\\b',\n",
        "        r'\\b\\d{1,2}(st|nd|rd|th)?\\s+of\\s+[A-Za-z]+,\\s+\\d{4}\\b',\n",
        "        r'\\b[A-Za-z]+\\s+\\d{1,2}(st|nd|rd|th)?,\\s+\\d{4}\\b'\n",
        "    ]\n",
        "\n",
        "    count_a = len(re.findall(a_pattern, text, re.IGNORECASE))\n",
        "    count_an = len(re.findall(an_pattern, text, re.IGNORECASE))\n",
        "    count_the = len(re.findall(the_pattern, text, re.IGNORECASE))\n",
        "\n",
        "    count_dates = 0\n",
        "    for pattern in date_patterns:\n",
        "        count_dates += len(re.findall(pattern, text))\n",
        "\n",
        "    return count_a, count_an, count_the, count_dates\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        t = int(input().strip())\n",
        "    except EOFError:\n",
        "        return\n",
        "\n",
        "    fragments = []\n",
        "\n",
        "    for _ in range(t):\n",
        "        try:\n",
        "            fragment = input().strip()\n",
        "            fragments.append(fragment)\n",
        "            input()\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "\n",
        "    results = []\n",
        "    for fragment in fragments:\n",
        "        count_a, count_an, count_the, count_dates = count_articles_and_dates(fragment)\n",
        "        results.extend([count_a, count_an, count_the, count_dates])\n",
        "\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BJWSYMSc2QXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Who is it?\n",
        "import re\n",
        "\n",
        "def resolve_pronouns(text, entities):\n",
        "    \"\"\"\n",
        "    Resolves highlighted pronouns in the text to corresponding entities.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text containing highlighted pronouns.\n",
        "        entities (list): List of possible entities.\n",
        "\n",
        "    Returns:\n",
        "        list: Resolved entities for each highlighted pronoun.\n",
        "    \"\"\"\n",
        "    pronoun_pattern = r'\\*\\*(.*?)\\*\\*'\n",
        "    pronouns = re.findall(pronoun_pattern, text)\n",
        "\n",
        "    entity_mentions = {}\n",
        "    resolved_entities = []\n",
        "\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for entity in entities:\n",
        "            if entity in sentence:\n",
        "                entity_mentions[entity] = len(entity_mentions)\n",
        "\n",
        "        for match in re.finditer(pronoun_pattern, sentence):\n",
        "            pronoun = match.group(1)\n",
        "\n",
        "            if pronoun.lower() in ['he', 'his', 'him']:\n",
        "                possible_entities = [e for e in entity_mentions if re.search(r'\\b[A-Z][a-z]+\\b', e)]\n",
        "            elif pronoun.lower() in ['she', 'her']:\n",
        "                possible_entities = [e for e in entity_mentions if re.search(r'\\b[A-Z][a-z]+\\b', e)]\n",
        "            elif pronoun.lower() in ['it', 'its']:\n",
        "                possible_entities = [e for e in entity_mentions if not re.search(r'\\b[A-Z][a-z]+\\b', e)]\n",
        "            else:\n",
        "                possible_entities = entities\n",
        "\n",
        "            if possible_entities:\n",
        "                resolved_entity = max(possible_entities, key=lambda x: entity_mentions.get(x, -1))\n",
        "                resolved_entities.append(resolved_entity)\n",
        "            else:\n",
        "                resolved_entities.append(\"Unknown\")\n",
        "\n",
        "    return resolved_entities\n",
        "\n",
        "\n",
        "def main():\n",
        "    n = int(input().strip())\n",
        "    text_lines = [input().strip() for _ in range(n)]\n",
        "    entities = input().strip().split(';')\n",
        "\n",
        "    text = \" \".join(text_lines)\n",
        "\n",
        "    results = resolve_pronouns(text, entities)\n",
        "\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-TfPu_9n2XFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}